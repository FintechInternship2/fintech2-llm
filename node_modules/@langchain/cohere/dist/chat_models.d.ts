import { CohereClient, Cohere } from "cohere-ai";
import { type BaseMessage } from "@langchain/core/messages";
import { BaseLanguageModelInput } from "@langchain/core/language_models/base";
import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { type BaseChatModelParams, BaseChatModel, LangSmithParams, BaseChatModelCallOptions, BindToolsInput } from "@langchain/core/language_models/chat_models";
import { ChatGenerationChunk, ChatResult } from "@langchain/core/outputs";
import { AIMessageChunk } from "@langchain/core/messages";
import { Runnable } from "@langchain/core/runnables";
type ChatCohereToolType = BindToolsInput | Cohere.Tool;
/**
 * Input interface for ChatCohere
 */
export interface ChatCohereInput extends BaseChatModelParams {
    /**
     * The API key to use.
     * @default {process.env.COHERE_API_KEY}
     */
    apiKey?: string;
    /**
     * The name of the model to use.
     * @default {"command"}
     */
    model?: string;
    /**
     * What sampling temperature to use, between 0.0 and 2.0.
     * Higher values like 0.8 will make the output more random,
     * while lower values like 0.2 will make it more focused
     * and deterministic.
     * @default {0.3}
     */
    temperature?: number;
    /**
     * Whether or not to stream the response.
     * @default {false}
     */
    streaming?: boolean;
    /**
     * Whether or not to include token usage when streaming.
     * This will include an extra chunk at the end of the stream
     * with `eventType: "stream-end"` and the token usage in
     * `usage_metadata`.
     * @default {true}
     */
    streamUsage?: boolean;
}
interface TokenUsage {
    completionTokens?: number;
    promptTokens?: number;
    totalTokens?: number;
}
export interface ChatCohereCallOptions extends BaseChatModelCallOptions, Partial<Omit<Cohere.ChatRequest, "message" | "tools">>, Partial<Omit<Cohere.ChatStreamRequest, "message" | "tools">>, Pick<ChatCohereInput, "streamUsage"> {
    tools?: ChatCohereToolType[];
}
/** @deprecated Import as ChatCohereCallOptions instead. */
export interface CohereChatCallOptions extends ChatCohereCallOptions {
}
/**
 * Integration with ChatCohere
 * @example
 * ```typescript
 * const model = new ChatCohere({
 *   apiKey: process.env.COHERE_API_KEY, // Default
 *   model: "command-r-plus" // Default
 * });
 * const response = await model.invoke([
 *   new HumanMessage("How tall are the largest pengiuns?")
 * ]);
 * ```
 */
export declare class ChatCohere<CallOptions extends ChatCohereCallOptions = ChatCohereCallOptions> extends BaseChatModel<CallOptions, AIMessageChunk> implements ChatCohereInput {
    static lc_name(): string;
    lc_serializable: boolean;
    client: CohereClient;
    model: string;
    temperature: number;
    streaming: boolean;
    streamUsage: boolean;
    constructor(fields?: ChatCohereInput);
    getLsParams(options: this["ParsedCallOptions"]): LangSmithParams;
    _llmType(): string;
    invocationParams(options: this["ParsedCallOptions"]): {
        [k: string]: string | number | CallOptions["preamble"] | CallOptions["conversationId"] | CallOptions["promptTruncation"] | CallOptions["connectors"] | CallOptions["searchQueriesOnly"] | CallOptions["documents"] | CallOptions["forceSingleStep"] | CallOptions["tools"] | undefined;
    };
    bindTools(tools: ChatCohereToolType[], kwargs?: Partial<CallOptions>): Runnable<BaseLanguageModelInput, AIMessageChunk, CallOptions>;
    /** @ignore */
    private _getChatRequest;
    private _getCurrChatTurnMessages;
    private _messagesToCohereToolResultsCurrChatTurn;
    private _messageToCohereToolResults;
    private _formatCohereToolCalls;
    private _convertCohereToolCallToLangchain;
    /** @ignore */
    _generate(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
    _streamResponseChunks(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;
    _combineLLMOutput(...llmOutputs: CohereLLMOutput[]): CohereLLMOutput;
    get lc_secrets(): {
        [key: string]: string;
    } | undefined;
    get lc_aliases(): {
        [key: string]: string;
    } | undefined;
}
interface CohereLLMOutput {
    estimatedTokenUsage?: TokenUsage;
}
export {};
